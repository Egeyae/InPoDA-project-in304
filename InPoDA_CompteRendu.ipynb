{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# [Projet InPoDA - IN304](https://github.com/Egeyae/InPoDA-project-in304) - UVSQ UFR DES SCIENCES\n",
    "#### *Done by KONSTANTINOV Julien (22301776) and COSSEC Elouan (22300813)*\n",
    "---\n",
    "**Goal:** Make a tweet analysis application *(extracting from french tweets: author, hastags, user mentioned, sentiment, topics)* and performing various data analysis actions\n",
    "\n",
    "**Table of Contents**:\n",
    "\n",
    "    - Part I: How we extract the tweets from the provided file\n",
    "    - Part II: Different analysis operations performed on the tweets\n",
    "    - Part III: Some references used for the project\n",
    "\n",
    "**Diagram**:\n",
    "\n",
    "![title](diagram/InPoDA_Diagram.drawio.png)"
   ],
   "id": "991cbd6ab9e8607a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Installation\n",
    "---\n",
    "For installation process, please follow the guide in the `README.md` found in the project directory.\n",
    "It is recommended to use a virtual environnement (with `Python 3.12.x` interpreter (or latest supported version by PyTorch))\n",
    "---\n",
    "After installing the environnement, you can run the following Jupyter Notebook"
   ],
   "id": "1caac56652e42fbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### Setup\n",
    "from InPoDA_Pipeline import *\n",
    "\n",
    "# The InPoDA_Pipeline class is used as an interface to use the project\n",
    "# A logger is set up automatically, to remove any logging/log to a file, please update the config.json file\n",
    "pipeline = InPoDAPipeline()\n",
    "pipeline.logger.info(\"Pipeline setup was a success\")"
   ],
   "id": "8a7a9a7e4136adca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## I - Tweets data extraction\n",
    "---\n",
    "This part is a detailed explanation on how we extract tweets and parse them into a pandas.Dataframe()"
   ],
   "id": "396b7d0f9fe2d35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### ***1.** Load the tweets in memory*",
   "id": "cd0298df786117c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tweets = pipeline.load_tweets()",
   "id": "5796dfe5c3a172b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pretty_dict_display(tweets)",
   "id": "b4581eecfe8c25f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### ***2.** Process the tweets in a pandas.DataFrame*\n",
    "\n",
    "By default, the Model used for topic classification is the smaller one. This is faster to run but can induce worse results in terms of topic identification. If you want to run the bigger model, update `config.json` and change the _`\"topic_model\":\"small\"`_ to _`\"topic_model\":\"big\"`_"
   ],
   "id": "91a96458e4ced64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Perform data extraction on the loaded tweets\n",
    "dataframe = pipeline.process_tweets_to_dataframe()"
   ],
   "id": "d99b6540177a1bb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataframe",
   "id": "5f6bfe39b5c39102"
  },
  {
   "cell_type": "markdown",
   "id": "591af8ec-d5c8-44fb-a265-fdf1ac85517f",
   "metadata": {},
   "source": [
    "##### *(**3.** Annex: Sentiment Analysis)*\n",
    "\n",
    "For learning purposes, we tried to create our own Neural Network model, trained to find the sentiment of a tweet. We used a Genetic Algorithm approach to explore solutions as we were not at ease with backpropagation. The training dataset is Sentiment140, around 1.6 millions tweets annoted for sentiment analysis. We embedded the training tweets using a multilingual model as the project tweets were in French and Sentiment140's are in english.\n",
    "\n",
    "In the following cells, we try to present the global pipeline of model usage and training. However, training the model can cost a lot in terms of resources, so the code is commented by default.\n",
    "\n",
    "PS: As the results were too bad for any practical usage, InPoDA uses textblob for the moment until we find a valid solution. The predictions are very off the expected results, we get all fed data to be more or less in the same category. Which is strange because during training everything seems fine... An error lies between training and model usage: during training we have almost perfect results but when testing the best model we only get 50% accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c32cc8-118e-4b39-8ef0-993ae9de900e",
   "metadata": {},
   "source": [
    "###### **a.** Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386fcdc6-2844-4478-a71c-da748b099f12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T15:54:36.163148471Z",
     "start_time": "2024-12-07T16:54:09.714009Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Loads the best pre-computed model\n",
    "pipeline.load_creature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3224487-79aa-43d6-9c00-156d7413b936",
   "metadata": {},
   "source": [
    "###### **b.** Model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be197a-6032-4ec0-9ee7-47f6758a66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the pre-computed model\n",
    "test_tweet = \"I'm so happy\"\n",
    "\n",
    "pipeline.process_input(test_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e0802-14e2-449d-89d8-dd42ff28f7d1",
   "metadata": {},
   "source": [
    "###### (**c.** Model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02069beb-6654-4ba7-8ab8-bc18f44aca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is very big and (1.6 million tweets) and it can be heavy on memory to store that much embeddings (768 * 2 bytes * 1.6 million ~= 2.3 GB)\n",
    "# To prevent this, it treats the data chunk by chunk and save those chunks onto the disk in order to load these chunks only when needed during training.\n",
    "# By default, chunks computation is deactivated as it can be expensive\n",
    "run_chunks = False\n",
    "\n",
    "if run_chunks: \n",
    "    pipeline.compute_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5119740-de82-4bf4-a9f3-1ba43974cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads as a pandas.DataFrame the training data (all computed chunks)\n",
    "# Configuration can be updated in the `config.json` file\n",
    "\n",
    "pipeline.load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf7cc2-3230-4caa-bfc9-70ecbc170238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is split in 2: first half is negative (sentiment 0)\n",
    "pipeline.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35217b-5363-4064-875e-ce3e3c899e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second half is positive (sentiment 4)\n",
    "pipeline.data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093ce08-d124-4b32-a64e-59eb15898af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we train a model based on the loaded data\n",
    "# Since it is very expensive, it doesn't run by default\n",
    "# Moreover, it is preferable to run directly the script `run_training.py` found in the ./sentiment_analysis/ folder\n",
    "# ! Be aware that you need to update the save file in `config.json` if you don't want to override pre-trained model !\n",
    "run = False\n",
    "\n",
    "if run:\n",
    "    pipeline.train_genetic_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e13327b843cce",
   "metadata": {},
   "source": [
    "## II - Tweets data analysis\n",
    "---\n",
    "This part is a presentation of different analysis we can do using the data we loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b78d1-0f33-4969-ab14-4674139029f7",
   "metadata": {},
   "source": [
    "#### **0. Data Presentation**\n",
    "---\n",
    "Presentation of all unique Authors, Mentions, Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34fdea-2e2e-4118-b8f7-56ba57b7c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All authors\n",
    "pipeline.get_all_authors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721b88d-25f2-45e6-899e-f3e557fb0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All mentions\n",
    "pipeline.get_all_mentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a9d40-e0c2-47a8-b1c2-af5ba37e1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All hashtags\n",
    "pipeline.get_all_hashtags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fbe9f-a3d9-4279-bc11-1f7a9a0cf3a8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "#### 1. **Top K analysis**\n",
    "---\n",
    "We extract:\n",
    "\n",
    "    - Top K hastags   (Most used hashtags)\n",
    "    - Top K authors   (Users who posted the most tweets)\n",
    "    - Top K mentioned (Users who were the most mentioned)\n",
    "    - Top K topics    (Topics that comme back the most)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c13b9-5259-489f-aadc-8c2f9b93e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the desired K value here\n",
    "K = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af19f37-152c-4e11-82a6-b47adc9a9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP K HASHTAGS\n",
    "pipeline.top_k_hashtags(k = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82ea06-3ed9-4535-b14b-1b9550ad2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP K AUTHORS\n",
    "pipeline.top_k_authors(k = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952edaf-69ec-43bf-93c3-f82e4a7c2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP K MENTIONED\n",
    "pipeline.top_k_mentioned(k = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7193aa-1938-4fb5-bde7-ea7bb2e087d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP K TOPICS\n",
    "pipeline.top_k_topics(k = K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9d78b-b9f8-4363-9b11-9a2594de43e5",
   "metadata": {},
   "source": [
    "#### **2. Number of tweets per X**\n",
    "---\n",
    "We extract:\n",
    "\n",
    "    - Number of tweets per user\n",
    "    - Number of tweets per hashtags\n",
    "    - Number of tweets per topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63ec78-f10b-48ba-9777-f8390a6918b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets per user\n",
    "pipeline.number_of_tweets_per_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a6c50-295d-407b-98f6-3f62d95ba57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets per hashtags\n",
    "pipeline.number_of_tweets_per_hashtag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8408ca-d86a-49b7-8db6-cb18f0c1432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets per topics\n",
    "pipeline.number_of_tweets_per_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775f8ce-d972-44ed-8230-4feacd5967cb",
   "metadata": {},
   "source": [
    "#### **3. User analysis**\n",
    "---\n",
    "We extract all tweets from a provided user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac0994-d5de-4118-87d9-bcfebc73abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set user\n",
    "user = pipeline.get_all_authors().iloc[0, 0]\n",
    "\n",
    "# All tweets from user\n",
    "pipeline.all_tweets_from_user(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28304e9-2555-4aa0-968e-4512f642d78d",
   "metadata": {},
   "source": [
    "#### **4. Usage analysis**\n",
    "---\n",
    "\n",
    "We extract:\n",
    "\n",
    "    - All tweets mentionning a specific user\n",
    "    - All users using a specific hashtag\n",
    "    - All users mentionned by a specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee6e47-bda7-47d8-9677-89e84bed2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tweets mentionning a specific user\n",
    "user = pipeline.get_all_mentions().loc[0]\n",
    "print(user.tolist())\n",
    "pipeline.all_tweets_where_user(user[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c73be-d73a-4ff0-828c-2fed92d0ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All users using a hashtag\n",
    "hashtag = pipeline.get_all_hashtags().loc[0]\n",
    "hashtag = hashtag.tolist()\n",
    "pipeline.all_users_using_hashtag(hashtag[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01abeb3-adfe-4e8d-a6b0-be0de2f894a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All users mentioned by a specific user\n",
    "user = \"372993152\" #pipeline.get_all_authors().iloc[0, 0]\n",
    "\n",
    "pipeline.all_users_mentioned_by_user(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee482de-7b11-4d13-8e74-b9737773cad8",
   "metadata": {},
   "source": [
    "## III - References\n",
    "---\n",
    "Dataset:\n",
    "`Sentiment140 dataset with 1.6 million tweets. (2017, September 13). https://www.kaggle.com/datasets/kazanova/sentiment140`\n",
    "\n",
    "Genetic Algorithm:\n",
    "`9. Evolutionary computing. (n.d.). https://natureofcode.com/genetic-algorithms/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
